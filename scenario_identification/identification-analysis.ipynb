{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Scenario Identification Notebook\n",
    "\n",
    "This notebook shows the scenario identification experiments we used.\n",
    "Note that this notebook uses data that we are not able to make fully public at this time. \n",
    "To run this notebook, it will be necessary to run the proper simulations and compute the appropriate aggregate measures for each cascade.\n",
    "We include this notebook in order to further reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.base import clone\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = 10 # size of bins to consider\n",
    "\n",
    "# expressions to identify column types\n",
    "num_expr = re.compile(\"infections_(\\d+)_to_(\\d+)\")\n",
    "out_degree_expr = re.compile(\"^out_degree_(\\d+)_to_(\\d+)\")\n",
    "bdry_expr = re.compile(\"boundary_out_degree_(\\d+)_to_(\\d+)\")\n",
    "path_ct_expr = re.compile(\"cnt_*\")\n",
    "avg_ct_expr = re.compile(\"avg_*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifies which columns correspond to which bucket sizes\n",
    "# We only use one bucket size - experiments found little difference with increased granularity\n",
    "def is_bucket(str_val, expr, size):\n",
    "    matches = expr.match(str_val)\n",
    "    if not matches:\n",
    "        return False\n",
    "    \n",
    "    lb = int(matches[1])\n",
    "    ub = int(matches[2])\n",
    "    \n",
    "    return ub - lb == size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_model(data, bin_size, estimator, path_expr, col_set=None, n_iter=10):\n",
    "    \n",
    "    \"\"\"\n",
    "    do 10 train/evaluate splits using the specified data, and set of\n",
    "    columns\n",
    "    \n",
    "    data - the aggregate data where each row is a cascade\n",
    "    bin_size - specifies the subset of binned columns to use (e.g. 5, 10)\n",
    "    estimator - an instantiated SkLearn classifier \n",
    "    path_expr - a regex indicating whether to use avg or count for labeled paths\n",
    "    col_set - a tuple indicating which sets of columns to use\n",
    "    \n",
    "    returns - a list of 10 (accuracy, trained_estimator) tuples\n",
    "    \"\"\"\n",
    "    \n",
    "    inf_cols = data.columns[data.columns.map(lambda x: is_bucket(x, num_expr, bin_size))]\n",
    "    path_cols = data.filter(regex=path_expr).columns\n",
    "    \n",
    "    # get all other columns\n",
    "    out_degree_expr = re.compile(\"^out_degree_(\\d+)_to_(\\d+)\")\n",
    "    path_len_cols = data.filter(regex=\"^path_len_*\").columns\n",
    "    out_degree_cols = data.columns[data.columns.map(lambda x: is_bucket(x, out_degree_expr, 2))]\n",
    "\n",
    "    feature_data = data[inf_cols.union(path_cols).union(path_len_cols).union(out_degree_cols)].copy()\n",
    "\n",
    "    norm_cols = inf_cols.union(out_degree_cols).union(path_len_cols)\n",
    "\n",
    "    feature_data[norm_cols] = feature_data[norm_cols].divide(data[inf_cols].sum(axis=1).values, axis=0)\n",
    "    \n",
    "    if col_set:\n",
    "        cols = pd.Index([])\n",
    "        if \"inf_cols\" in col_set:\n",
    "            cols = cols.union(inf_cols)\n",
    "        if \"labeled_path_1\" in col_set:\n",
    "            labeled_path_1_ind = ~data.filter(regex=path_expr).columns.str.contains(\"-\")\n",
    "            cols = cols.union(data.filter(regex=path_expr).columns[labeled_path_1_ind])\n",
    "        if \"labeled_path_2\" in col_set:\n",
    "            labeled_path_2_ind = data.filter(regex=path_expr).columns.str.contains(\"-\")\n",
    "            cols = cols.union(data.filter(regex=path_expr).columns[labeled_path_2_ind])\n",
    "        if \"out_degree\" in col_set:\n",
    "            cols = cols.union(out_degree_cols)\n",
    "        if \"path_len\" in col_set:\n",
    "            cols = cols.union(path_len_cols)\n",
    "        feature_data = feature_data[cols]\n",
    "\n",
    "    runs = []\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(feature_data, \n",
    "                                                        data[\"label_scenario\"], shuffle=True, \n",
    "                                                        stratify=data[\"label_scenario\"], test_size=0.25)\n",
    "        estimator.fit(X_train, y_train)\n",
    "        acc = estimator.score(X_test, y_test)\n",
    "        \n",
    "        est_arch = deepcopy(estimator)\n",
    "      \n",
    "        runs.append((acc, est_arch))\n",
    "        #print(f\"Iteration {i}, {acc}\")\n",
    "\n",
    "    return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "\n",
    "col_combos = [\n",
    "    (\"inf_cols\", \"path_len\", \"out_degree\", \"labeled_path_1\", \"labeled_path_2\"), # Epicurve + structure\n",
    "    (\"inf_cols\",) # Epicurve only\n",
    "]\n",
    "\n",
    "# whether to use count or average, determined via exploratory testing\n",
    "lr_path_expr = \"cnt_en_*\" \n",
    "rf_path_expr = \"avg_en_*\"\n",
    "svm_path_expr = \"avg_s_en_*\"\n",
    "\n",
    "\n",
    "lr = make_pipeline(LogisticRegression(max_iter=10000))\n",
    "gr_lr = GridSearchCV(lr, param_grid={\n",
    "            \"logisticregression__C\" : np.arange(0.1, 3.0, 0.05)\n",
    "        }, n_jobs=4, cv=5)\n",
    "\n",
    "cv_rf = GridSearchCV(RandomForestClassifier(), param_grid={\n",
    "    \"max_features\" : [\"sqrt\", \"log2\"] + list(range(1,40)),\n",
    "}, n_jobs=4, cv=5)\n",
    "\n",
    "cv_svm = GridSearchCV(make_pipeline(StandardScaler(), SVC(kernel=\"linear\", probability=True)), param_grid={\n",
    "    \"svc__C\" : [1e-2, 1e-1, 1.0, 1e1],    \n",
    "}, n_jobs=4, cv=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "directory = \"/project/biocomplexity/cascade_analytics/2023-KDD/aggregates/exp4.coverage_l1/\"\n",
    "file_expr = re.compile(\"cells-1-3-5-7_coverage-(\\d\\.\\d)_seed-(\\d).csv\")\n",
    "\n",
    "files = glob(directory+\"*.csv\")\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in files:\n",
    "    meta = file_expr.match(os.path.basename(file))\n",
    "    coverage = float(meta[1])\n",
    "    seed = int(meta[2])\n",
    "    \n",
    "    df = pd.read_csv(file)\n",
    "    df[\"coverage\"] = coverage\n",
    "    df[\"seed\"] = seed\n",
    "    \n",
    "    dfs.append(df)\n",
    "    \n",
    "cov_data = pd.concat(dfs)\n",
    "cov_data.set_index([\"coverage\", \"seed\"], inplace=True)\n",
    "cov_data.fillna(0.0, inplace=True) # missing appears to be because of incompatible columns due to binning\n",
    "cov_data.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression results\n",
    "\n",
    "lr_results = pd.DataFrame(columns = [\"struct\",\"epicurve\"],\n",
    "                         index=pd.MultiIndex.from_product([\n",
    "                             [0.6,0.7,0.8,0.9],\n",
    "                             [1,2,3,4,5],\n",
    "                             [0,1,2,3,4,5,6,7,8,9]\n",
    "                         ]))\n",
    "\n",
    "for coverage in cov_data.index.get_level_values(0).unique():\n",
    "    for seed in cov_data.index.get_level_values(1).unique():\n",
    "        data = cov_data.loc[(coverage, seed),:].reset_index().drop([\"coverage\", \"seed\"], axis=1)\n",
    "        full_res = train_eval_model(data, 10, gr_lr, re.compile(lr_path_expr), col_set=col_combos[0])\n",
    "        epi_only = train_eval_model(data, 10, gr_lr, re.compile(lr_path_expr), col_set=col_combos[1])\n",
    "        \n",
    "        lr_results.loc[(coverage, seed, slice(None)), \"struct\"] = [r[0] for r in full_res]\n",
    "        lr_results.loc[(coverage, seed, slice(None)), \"epicurve\"] = [r[0] for r in epi_only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest results\n",
    "\n",
    "rf_results = pd.DataFrame(columns = [\"struct\",\"epicurve\"],\n",
    "                         index=pd.MultiIndex.from_product([\n",
    "                             [0.6,0.7,0.8,0.9],\n",
    "                             [1,2,3,4,5],\n",
    "                             [0,1,2,3,4,5,6,7,8,9]\n",
    "                         ]))\n",
    "\n",
    "for coverage in cov_data.index.get_level_values(0).unique():\n",
    "    for seed in cov_data.index.get_level_values(1).unique():\n",
    "        data = cov_data.loc[(coverage, seed),:].reset_index().drop([\"coverage\", \"seed\"], axis=1)\n",
    "        full_res = train_eval_model(data, 10, cv_rf, re.compile(rf_path_expr), col_set=col_combos[0])\n",
    "        epi_only = train_eval_model(data, 10, cv_rf, re.compile(rf_path_expr), col_set=col_combos[1])\n",
    "        \n",
    "        rf_results.loc[(coverage, seed, slice(None)), \"struct\"] = [r[0] for r in full_res]\n",
    "        rf_results.loc[(coverage, seed, slice(None)), \"epicurve\"] = [r[0] for r in epi_only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM results\n",
    "\n",
    "svm_results = pd.DataFrame(columns = [\"struct\",\"epicurve\"],\n",
    "                         index=pd.MultiIndex.from_product([\n",
    "                             [0.6,0.7,0.8,0.9],\n",
    "                             [1,2,3,4,5],\n",
    "                             [0,1,2,3,4,5,6,7,8,9]\n",
    "                         ]))\n",
    "\n",
    "for coverage in cov_data.index.get_level_values(0).unique():\n",
    "    for seed in cov_data.index.get_level_values(1).unique():\n",
    "        data = cov_data.loc[(coverage, seed),:].reset_index().drop([\"coverage\", \"seed\"], axis=1)\n",
    "        full_res = train_eval_model(data, 10, cv_svm, re.compile(svm_path_expr), col_set=col_combos[0])\n",
    "        epi_only = train_eval_model(data, 10, cv_svm, re.compile(svm_path_expr), col_set=col_combos[1])\n",
    "        \n",
    "        svm_results.loc[(coverage, seed, slice(None)), \"struct\"] = [r[0] for r in full_res]\n",
    "        svm_results.loc[(coverage, seed, slice(None)), \"epicurve\"] = [r[0] for r in epi_only]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dir = \"/project/biocomplexity/cascade_analytics/2023-KDD/aggregates\"\n",
    "\n",
    "t_50_file = \"exp4.T50.cell-1-3-5-7_outdegree-bins-1-2-10_boundary-outdegree-bins-1-5-10_epicurve-bin-1-5-10.csv\"\n",
    "data_50 = pd.read_csv(time_dir + t_50_file)\n",
    "\n",
    "\n",
    "t_70_file = \"exp4.T70.cell-1-3-5-7_outdegree-bins-1-2-10_boundary-outdegree-bins-1-5-10_epicurve-bin-1-5-10.csv\"\n",
    "data_70 = pd.read_csv(time_dir + t_70_file)\n",
    "\n",
    "t_90_file = \"exp4.T90.cell-1-3-5-7_outdegree-bins-1-2-10_boundary-outdegree-bins-1-5-10_epicurve-bin-1-5-10.csv\"\n",
    "data_90 = pd.read_csv(time_dir + t_90_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results = []\n",
    "rf_results = []\n",
    "svm_results = []\n",
    "\n",
    "for data in [data_50, data_70, data_90]:\n",
    "    for combo in col_combos:\n",
    "        rf_results.append(train_eval_model(data, 10, cv_rf, re.compile(rf_path_expr),col_set=combo, n_iter=2))\n",
    "        svm_results.append(train_eval_model(data, 10, cv_svm, re.compile(svm_path_expr),col_set=combo, n_iter=2))\n",
    "        lr_results.append(train_eval_model(data, 10, gr_lr, re.compile(lr_path_expr), col_set=col, n_iter=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find max model over all training runs, this is used for SHAP analysis\n",
    "\n",
    "model_dict = {} # map lr -> time -> model\n",
    "for model_name, model_result in zip([\"LR\", \"RF\", \"SVM\"], [lr_results, rf_results, svm_results]):\n",
    "    i = 0\n",
    "    model_dict[model_name] = {}\n",
    "    \n",
    "    for t in [\"50\", \"70\", \"90\"]:\n",
    "        for col in [\"Epicurve+Structure\", \"Epicurve Only\"]:\n",
    "            if col != \"Epicurve Only\":\n",
    "                result = model_result[i].copy()\n",
    "                result.sort(key = lambda x: x[0])\n",
    "                \n",
    "                model_dict[model_name][t] = result[-1][1]\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import altair as alt\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "# To compute SHAP, we need feature subsets that were used to train each model\n",
    "# \n",
    "\n",
    "def get_data(data, path_expr):\n",
    "    \n",
    "    inf_cols = data.columns[data.columns.map(lambda x: is_bucket(x, num_expr, 10))]\n",
    "    path_cols = data.filter(regex=path_expr).columns\n",
    "    # get all other columns\n",
    "    out_degree_expr = re.compile(\"^out_degree_(\\d+)_to_(\\d+)\")\n",
    "    path_len_cols = data.filter(regex=\"^path_len_*\").columns\n",
    "    out_degree_cols = data.columns[data.columns.map(lambda x: is_bucket(x, out_degree_expr, 2))]\n",
    "\n",
    "    feature_data = data[inf_cols.union(path_cols).union(path_len_cols).union(out_degree_cols)].copy()\n",
    "\n",
    "    norm_cols = inf_cols.union(out_degree_cols).union(path_len_cols)\n",
    "\n",
    "    feature_data[norm_cols] = feature_data[norm_cols].divide(data[inf_cols].sum(axis=1).values, axis=0)\n",
    "    \n",
    "    return feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take SHAP measures for each row/column and average over the absolute value by feature-group\n",
    "\n",
    "def arr_shap_df(raw_shap, path_expr):\n",
    "    # return a dataframe mapping groups of columns to \n",
    "    # avg of abs. cols\n",
    "    labeled_edge_idx = ~raw_shap.filter(regex=path_expr).droplevel(0, axis=1).columns.str.contains(\"-\")\n",
    "    labeled_edge_cols = raw_shap.filter(regex=path_expr).droplevel(0, axis=1).columns[labeled_edge_idx]\n",
    "\n",
    "    labeled_path_cols = raw_shap.filter(regex=path_expr).droplevel(0, axis=1).columns[~labeled_edge_idx]\n",
    "    path_len_cols = raw_shap.droplevel(0, axis=1).filter(regex=\"^path_len_*\").columns\n",
    "\n",
    "    out_degree_cols = raw_shap.droplevel(0, axis=1).columns[raw_shap.columns.map(lambda x: is_bucket(x[1], out_degree_expr, 2))]\n",
    "    epicurve_cols = raw_shap.droplevel(0, axis=1).columns[raw_shap.columns.map(lambda x: is_bucket(x[1], num_expr, 10))]\n",
    "\n",
    "    plot_data = {}\n",
    "    \n",
    "    for group_name, col_set in zip([\"Labeled Edges\", \"Labeled paths\", \n",
    "                                    \"Path length\", \"Outdegree\", \"Epicurve\"],\n",
    "                    [labeled_edge_cols, labeled_path_cols, \n",
    "                    path_len_cols, out_degree_cols, epicurve_cols]):\n",
    "        col_data = raw_shap.loc[:,(slice(None), col_set)].abs().mean(axis=1)\n",
    "        plot_data[group_name] = col_data\n",
    "        \n",
    "    return pd.DataFrame(plot_data)\n",
    "\n",
    "# create a plot of the spread by time\n",
    "def plot_feature_time(shap_df, path_expr, feature_group, color):\n",
    "    \n",
    "    \"\"\"\n",
    "    return altair chart showing change in mean abs value shap over time for feature_group\n",
    "    \"\"\"\n",
    "    time_dfs = []\n",
    "    for time in shap_df.index.get_level_values(\"time\").unique():\n",
    "        time_df = arr_shap_df(shap_df.loc[time, :], path_expr)\n",
    "        time_df[\"time\"] = time\n",
    "        time_dfs.append(time_df)\n",
    "    chart_data = pd.concat(time_dfs)[[\"time\", feature_group]]\n",
    "    chart = alt.Chart(chart_data).mark_boxplot(color=color).encode(\n",
    "        x = alt.X(\"time\", scale=alt.Scale(domain=(40,100))),\n",
    "        y = alt.Y(feature_group, scale=alt.Scale(domain=(0,0.2)), title=\"|Mean Shapley|\"),\n",
    "    ).properties(\n",
    "        title=feature_group\n",
    "    )\n",
    "    \n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually get the data\n",
    "\n",
    "lr_data_50 = get_data(data_50, lr_path_expr)\n",
    "lr_data_70 = get_data(data_70, lr_path_expr)\n",
    "lr_data_90 = get_data(data_90, lr_path_expr)\n",
    "\n",
    "rf_data_50 = get_data(data_50, rf_path_expr)\n",
    "rf_data_70 = get_data(data_70, rf_path_expr)\n",
    "rf_data_90 = get_data(data_90, rf_path_expr)\n",
    "\n",
    "svm_data_50 = get_data(data_50, svm_path_expr)\n",
    "svm_data_70 = get_data(data_70, svm_path_expr)\n",
    "svm_data_90 = get_data(data_90, svm_path_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR SHAP computation\n",
    "\n",
    "all_cols = lr_data_50.columns.union(lr_data_70.columns).union(lr_data_90.columns)\n",
    "\n",
    "lr_shap_df = pd.DataFrame(\n",
    "    index = pd.MultiIndex.from_product([[50, 70, 90],\n",
    "                                      svm_data_50.index], names=[\"time\", \"cascade\"]),\n",
    "    columns = pd.MultiIndex.from_product([\n",
    "        model_dict[\"LR\"][\"50\"].classes_,\n",
    "        all_cols],names=[\"class\", \"feature\"])\n",
    ")\n",
    "\n",
    "\n",
    "for (time, model),data in zip(model_dict[\"LR\"].items(), [lr_data_50, lr_data_70, lr_data_90]):\n",
    "    \n",
    "    explainer = shap.Explainer(model.predict_proba, data)\n",
    "    shap_values = explainer(data)\n",
    "    \n",
    "    for i,class_val in enumerate(model.classes_):\n",
    "        lr_shap_df.loc[(int(time), slice(None)), (class_val, data.columns)] = shap_values[:,:,i].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF SHAP computation\n",
    "\n",
    "all_cols = rf_data_50.columns.union(rf_data_70.columns).union(rf_data_90.columns)\n",
    "rf_shap_df = pd.DataFrame(\n",
    "    index = pd.MultiIndex.from_product([[50, 70, 90],\n",
    "                                      rf_data_50.index], names=[\"time\", \"cascade\"]),\n",
    "    columns = pd.MultiIndex.from_product([\n",
    "        model_dict[\"RF\"][\"50\"].classes_,\n",
    "        all_cols],names=[\"class\", \"feature\"])\n",
    ")\n",
    "# generate shap values for each model and time\n",
    "# put into df of time x model x cell x feature\n",
    "\n",
    "# rf\n",
    "\n",
    "for (time, model),data in zip(model_dict[\"RF\"].items(), [rf_data_50, rf_data_70, rf_data_90]):\n",
    "    \n",
    "    explainer = shap.Explainer(model.best_estimator_)\n",
    "    shap_values = explainer(data)\n",
    "    \n",
    "    for i,class_val in enumerate(model.classes_):\n",
    "        rf_shap_df.loc[(int(time), slice(None)), (class_val, data.columns)] = shap_values[:,:,i].values\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM SHAP computation\n",
    "\n",
    "all_cols = svm_data_50.columns.union(svm_data_70.columns).union(svm_data_90.columns)\n",
    "\n",
    "svm_shap_df = pd.DataFrame(\n",
    "    index = pd.MultiIndex.from_product([[50, 70, 90],\n",
    "                                      svm_data_50.index], names=[\"time\", \"cascade\"]),\n",
    "    columns = pd.MultiIndex.from_product([\n",
    "        model_dict[\"SVM\"][\"50\"].classes_,\n",
    "        all_cols],names=[\"class\", \"feature\"])\n",
    ")\n",
    "\n",
    "\n",
    "for (time, model),data in zip(model_dict[\"SVM\"].items(), [svm_data_50, svm_data_70, svm_data_90]):\n",
    "    \n",
    "    explainer = shap.Explainer(model.predict_proba, data)\n",
    "    shap_values = explainer(data)\n",
    "    \n",
    "    for i,class_val in enumerate(model.classes_):\n",
    "        svm_shap_df.loc[(int(time), slice(None)), (class_val, data.columns)] = shap_values[:,:,i].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {\n",
    "    \"Labeled Edges\" : \"#1b9e77\",\n",
    "    \"Labeled paths\" : \"#d95f02\",\n",
    "    \"Path length\" : \"#7570b3\",\n",
    "    \"Outdegree\" : \"#e7298a\",\n",
    "    \"Epicurve\" : \"#66a61e\"\n",
    "}\n",
    "\n",
    "lr_chart = alt.hconcat().properties(title=\"LR\")\n",
    "rf_chart = alt.hconcat().properties(title=\"RF\")\n",
    "svm_chart = alt.hconcat().properties(title=\"SVM\")\n",
    "\n",
    "for feature_group in color_map:\n",
    "    lr_chart |= plot_feature_time(lr_shap_df, lr_path_expr, feature_group, color_map[feature_group])\n",
    "    rf_chart |= plot_feature_time(rf_shap_df, rf_path_expr, feature_group, color_map[feature_group])\n",
    "    svm_chart |= plot_feature_time(svm_shap_df, svm_path_expr, feature_group, color_map[feature_group])\n",
    "    \n",
    "    \n",
    "lr_chart\n",
    "rf_chart\n",
    "svm_chart\n",
    "\n",
    "lr_chart & rf_chart & svm_chart"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
